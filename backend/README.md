# ğŸ¤Ÿ Backend - Sign Language Recognition

Backend WebSocket pour la reconnaissance de langue des signes en temps rÃ©el.

## ğŸ“‹ PrÃ©requis

- Python 3.10+
- ngrok (pour exposer le serveur)

## ğŸš€ Installation

### 1. CrÃ©er un environnement virtuel

```bash
cd backend
python -m venv venv

# Windows
venv\Scripts\activate

# macOS/Linux
source venv/bin/activate
```

### 2. Installer les dÃ©pendances

```bash
pip install -r requirements.txt
```

### 3. Lancer le serveur

```bash
python main.py
```

Le serveur dÃ©marre sur `http://localhost:8000`

### 4. Exposer avec ngrok

Dans un autre terminal :

```bash
ngrok http 8000
```

Tu obtiendras une URL du type : `https://abc123.ngrok.io`

**âš ï¸ Important** : Pour le frontend, utilise `wss://abc123.ngrok.io/ws` (avec `wss://` et `/ws`)

## ğŸ“¡ Contrat WebSocket

### Endpoint

```
ws://localhost:8000/ws
# ou via ngrok:
wss://abc123.ngrok.io/ws
```

### Messages reÃ§us (du frontend)

Le frontend envoie **25 frames par seconde** :

```json
{
  "type": "frame",
  "data": "data:image/jpeg;base64,/9j/4AAQSkZJRg...",
  "timestamp": 1702834567890
}
```

- `type`: Toujours "frame"
- `data`: Image JPEG encodÃ©e en base64 (640x480, qualitÃ© 70%)
- `timestamp`: Timestamp Unix en millisecondes

### Messages envoyÃ©s (vers le frontend)

Envoie le mot reconnu Ã  **chaque frame** (le frontend gÃ¨re la dÃ©duplication) :

```json
{
  "type": "word",
  "word": "Hello"
}
```

- `type`: Toujours "word"
- `word`: Le mot reconnu par le modÃ¨le ML

**Note importante** : Le frontend filtre automatiquement les mots rÃ©pÃ©tÃ©s. Tu peux donc envoyer le mÃªme mot plusieurs fois, seul le premier sera traitÃ© et envoyÃ© au TTS.

## ğŸ§  IntÃ©gration du modÃ¨le ML

Modifie la fonction `recognize_sign()` dans `main.py` :

```python
def recognize_sign(frame_data: str) -> str | None:
    # 1. Extraire l'image base64
    base64_string = frame_data.split(",")[1]
    image_bytes = base64.b64decode(base64_string)
    
    # 2. Convertir en format utilisable (exemple avec OpenCV)
    import numpy as np
    import cv2
    nparr = np.frombuffer(image_bytes, np.uint8)
    image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
    
    # 3. Passer au modÃ¨le ML
    word = ton_modele.predict(image)
    
    return word  # ou None si rien dÃ©tectÃ©
```

## ğŸ“ Structure

```
backend/
â”œâ”€â”€ main.py           # Serveur FastAPI + WebSocket
â”œâ”€â”€ requirements.txt  # DÃ©pendances Python
â””â”€â”€ README.md         # Ce fichier
```

## ğŸ”§ Configuration frontend

Le frontend doit configurer cette variable d'environnement :

```bash
# .env.local (cÃ´tÃ© frontend Next.js)
NEXT_PUBLIC_WS_URL=wss://abc123.ngrok.io
```

## ğŸ“Š SpÃ©cifications techniques

| ParamÃ¨tre | Valeur |
|-----------|--------|
| Frame rate | 25 FPS |
| RÃ©solution | 640x480 |
| Format image | JPEG base64 |
| QualitÃ© JPEG | 70% |
| Intervalle | 40ms |

## ğŸ§ª Test rapide

1. Lance le serveur : `python main.py`
2. Ouvre `http://localhost:8000` â†’ doit afficher `{"status": "ok", ...}`
3. Le mode mock renvoie des mots alÃ©atoires pour tester la connexion

## ğŸ”„ Flux complet

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      25 FPS       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FRONTEND   â”‚ â”€â”€ frames â”€â”€â”€â”€â”€â”€â–º â”‚   BACKEND   â”‚
â”‚  (Next.js)  â”‚                   â”‚  (FastAPI)  â”‚
â”‚             â”‚ â—„â”€â”€ mots â”€â”€â”€â”€â”€â”€â”€â”€ â”‚   + ML      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ mot unique (filtrÃ©)
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ELEVENLABS  â”‚
â”‚   (TTS)     â”‚
â”‚      ğŸ”Š     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Notes

- Le frontend filtre les mots rÃ©pÃ©tÃ©s â†’ tu peux renvoyer le mÃªme mot Ã  chaque frame
- 25 frames/seconde = ton modÃ¨le doit traiter une frame en < 40ms idÃ©alement
- En production, remplace `allow_origins=["*"]` par l'URL exacte du frontend
